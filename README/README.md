# Client-Server ë¶„ë¦¬ ì•„í‚¤í…ì²˜ êµ¬í˜„ ê°€ì´ë“œ

## ê°œìš”
Clientì—ì„œ ì „ì²˜ë¦¬ì™€ Vision Encoderë¥¼ ì‹¤í–‰í•˜ê³ , Serverì—ì„œëŠ” LLM Prefill/Decodingë§Œ ìˆ˜í–‰í•˜ë„ë¡ ë¶„ë¦¬í•©ë‹ˆë‹¤.

## í•„ìš”í•œ ì½”ë“œ ìœ„ì¹˜

### 1. Client Side (On-Device) - ì „ì²˜ë¦¬ ì½”ë“œ

#### ğŸ“ `qwen-vl-utils/src/qwen_vl_utils/vision_process.py`
Clientì—ì„œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ì— í•„ìš”í•œ ëª¨ë“  í•¨ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ê°€ì ¸ì™€ì•¼ í•  ì£¼ìš” í•¨ìˆ˜ë“¤:**

```python
# 1. ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
- smart_resize(height, width, factor, min_pixels, max_pixels)  # ë™ì  í•´ìƒë„ ì¡°ì •
- to_rgb(pil_image)  # RGB ë³€í™˜
- round_by_factor(), ceil_by_factor(), floor_by_factor()  # í•´ìƒë„ ê³„ì‚°

# 2. ì´ë¯¸ì§€ ì²˜ë¦¬
- fetch_image(ele, image_patch_size)  # ì´ë¯¸ì§€ ë¡œë“œ & ë¦¬ì‚¬ì´ì¦ˆ

# 3. ë¹„ë””ì˜¤ ì²˜ë¦¬
- smart_nframes(ele, total_frames, video_fps)  # í”„ë ˆì„ ìˆ˜ ê³„ì‚°
- calculate_video_frame_range()  # í”„ë ˆì„ ë²”ìœ„ ê³„ì‚°
- _read_video_torchvision(ele)  # torchvisionìœ¼ë¡œ ë¹„ë””ì˜¤ ì½ê¸°
- _read_video_decord(ele)  # decordë¡œ ë¹„ë””ì˜¤ ì½ê¸° (ì„ íƒ)
- _read_video_torchcodec(ele)  # torchcodecë¡œ ë¹„ë””ì˜¤ ì½ê¸° (ì„ íƒ)
- fetch_video(ele, image_patch_size)  # ë¹„ë””ì˜¤ ë¡œë“œ & ë¦¬ì‚¬ì´ì¦ˆ

# 4. í†µí•© í•¨ìˆ˜
- extract_vision_info(conversations)  # ë©”ì‹œì§€ì—ì„œ vision ì •ë³´ ì¶”ì¶œ
- process_vision_info(conversations, image_patch_size)  # ì „ì²´ vision ì²˜ë¦¬
```

**íŒŒì¼ ìœ„ì¹˜:** `/home/shjung/Qwen3-VL-Agent/qwen-vl-utils/src/qwen_vl_utils/vision_process.py`

---

#### ğŸ“ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Processor
Clientì—ì„œ tokenizationì— í•„ìš”í•©ë‹ˆë‹¤.

```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# í•„ìš”í•œ ë©”ì„œë“œ:
- processor.apply_chat_template()  # ë©”ì‹œì§€ â†’ í…ìŠ¤íŠ¸ ë³€í™˜
- processor.tokenizer.encode()  # í…ìŠ¤íŠ¸ â†’ input_ids
- processor.image_processor  # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
- processor(text, images, videos)  # í†µí•© ì „ì²˜ë¦¬
```

---

### 2. Vision Encoder ì½”ë“œ

Vision EncoderëŠ” **transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**ì— êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### ğŸ” Vision Encoder ëª¨ë¸ êµ¬ì¡°

```python
# transformers.models.qwen3_vl.modeling_qwen3_vl.py (transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ë¶€)

class Qwen3VLVisionModel:
    """Vision Encoder (ViT)"""
    def __init__(self, config):
        self.patch_embed = ...  # Patch embedding
        self.blocks = nn.ModuleList([...])  # ViT transformer blocks
        self.merger = Qwen3VLMerger(config)  # DeepStack
    
    def forward(self, pixel_values, grid_thw):
        # 1. Patch embedding
        x = self.patch_embed(pixel_values)
        
        # 2. ViT blocks
        for block in self.blocks:
            x = block(x)
        
        # 3. DeepStack merger (multi-level feature fusion)
        vision_outputs = self.merger(x)
        
        return vision_outputs


class Qwen3VLForConditionalGeneration:
    """ì „ì²´ ëª¨ë¸"""
    def __init__(self, config):
        self.visual = Qwen3VLVisionModel(config)  # Vision Encoder
        self.language_model = Qwen3Model(config)  # LLM
        self.lm_head = nn.Linear(...)  # Output head
```

#### ğŸ“¦ Vision Encoderë§Œ ì¶”ì¶œí•˜ëŠ” ë°©ë²•

```python
from transformers import Qwen3VLForConditionalGeneration

# ì „ì²´ ëª¨ë¸ ë¡œë“œ
full_model = Qwen3VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct"
)

# Vision Encoderë§Œ ì¶”ì¶œ
vision_encoder = full_model.visual  # Qwen3VLVisionModel

# Vision Encoder Forward
vision_outputs = vision_encoder(
    pixel_values=pixel_values,  # [batch, channels, height, width]
    grid_thw=image_grid_thw  # [num_images, 3] - (T, H, W)
)
# ì¶œë ¥: vision_outputs.shape = [num_patches, hidden_dim]
```

---

### 3. Server Side - LLMë§Œ í•„ìš”

Serverì—ì„œëŠ” Vision Encoderì˜ ì¶œë ¥(vision embeddings)ì„ ë°›ì•„ì„œ LLMë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.

#### í•„ìš”í•œ ì½”ë“œ:

```python
# transformers.models.qwen3_vl.modeling_qwen3_vl.py

class Qwen3VLForConditionalGeneration:
    def forward(
        self,
        input_ids,
        attention_mask,
        pixel_values=None,  # Clientì—ì„œ ì²˜ë¦¬í•˜ë©´ ë¶ˆí•„ìš”
        image_grid_thw=None,  # Clientì—ì„œ ì²˜ë¦¬í•˜ë©´ ë¶ˆí•„ìš”
        vision_embeddings=None,  # Clientì—ì„œ ì „ë‹¬ë°›ìŒ (ìƒˆë¡œ ì¶”ê°€ í•„ìš”)
        **kwargs
    ):
        # Vision Encoder ê±´ë„ˆë›°ê³  ë°”ë¡œ LLMìœ¼ë¡œ
        if vision_embeddings is not None:
            # Clientì—ì„œ ë°›ì€ vision embeddings ì‚¬ìš©
            inputs_embeds = self._merge_vision_embeddings(
                input_ids, vision_embeddings
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (Serverì—ì„œ Vision Encoder ì‹¤í–‰)
            vision_outputs = self.visual(pixel_values, image_grid_thw)
            inputs_embeds = self._merge_vision_embeddings(
                input_ids, vision_outputs
            )
        
        # LLM Forward
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
        )
        
        return outputs
```

---

## êµ¬ì²´ì ì¸ êµ¬í˜„ ë‹¨ê³„

### Step 1: Client ì „ì²˜ë¦¬ ëª¨ë“ˆ ìƒì„±

```python
# client_preprocessor.py

from qwen_vl_utils import (
    extract_vision_info,
    fetch_image,
    fetch_video,
    process_vision_info,
    smart_resize
)
from transformers import AutoProcessor
import torch

class ClientPreprocessor:
    def __init__(self, model_name):
        self.processor = AutoProcessor.from_pretrained(model_name)
        
    def preprocess(self, messages):
        """Clientì—ì„œ ì „ì²´ ì „ì²˜ë¦¬ ìˆ˜í–‰"""
        
        # 1. Vision ì •ë³´ ì²˜ë¦¬
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        images, videos, video_kwargs = process_vision_info(
            messages,
            image_patch_size=self.processor.image_processor.patch_size,
            return_video_kwargs=True,
            return_video_metadata=True
        )
        
        # 2. Tokenization
        inputs = self.processor(
            text=text,
            images=images,
            videos=videos,
            do_resize=False,
            return_tensors="pt",
            **video_kwargs
        )
        
        return inputs
```

**í•„ìš”í•œ íŒŒì¼:**
- âœ… `qwen-vl-utils/src/qwen_vl_utils/vision_process.py` (ì „ì²´ ë³µì‚¬)
- âœ… `qwen-vl-utils/src/qwen_vl_utils/__init__.py`

---

### Step 2: Client Vision Encoder ì‹¤í–‰

```python
# client_vision_encoder.py

from transformers import Qwen3VLForConditionalGeneration
import torch

class ClientVisionEncoder:
    def __init__(self, model_name, device='cuda'):
        # ì „ì²´ ëª¨ë¸ ë¡œë“œ (Vision Encoderë§Œ ì‚¬ìš©)
        full_model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map=device
        )
        
        # Vision Encoderë§Œ ì¶”ì¶œ
        self.vision_encoder = full_model.visual
        self.vision_encoder.eval()
        
    @torch.no_grad()
    def encode(self, pixel_values, image_grid_thw, video_grid_thw=None):
        """Vision Encoder Forward"""
        
        # Vision Encoder ì‹¤í–‰
        vision_outputs = self.vision_encoder(
            pixel_values=pixel_values,
            grid_thw=torch.cat([image_grid_thw, video_grid_thw]) if video_grid_thw else image_grid_thw
        )
        
        return vision_outputs  # [num_patches, hidden_dim]
```

**í•„ìš”í•œ ì½”ë“œ:**
- âœ… `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install transformers)
- âœ… Vision Encoder ë¶€ë¶„ë§Œ ì¶”ì¶œ (full_model.visual)

---

### Step 3: Server LLM ì „ìš© ëª¨ë“ˆ

```python
# server_llm_inference.py

from transformers import Qwen3VLForConditionalGeneration
import torch

class ServerLLMInference:
    def __init__(self, model_name):
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto",
            attn_implementation="flash_attention_2"
        )
        self.model.eval()
        
    def generate_from_embeddings(
        self,
        input_ids,
        vision_embeddings,
        vision_token_positions,
        attention_mask=None,
        max_new_tokens=128
    ):
        """Vision embeddingsì„ ë°›ì•„ì„œ LLMë§Œ ì‹¤í–‰"""
        
        # Text embeddings
        text_embeds = self.model.language_model.embed_tokens(input_ids)
        
        # Vision embeddings ì‚½ì…
        inputs_embeds = self._merge_embeddings(
            text_embeds, vision_embeddings, vision_token_positions
        )
        
        # LLM Generation
        with torch.no_grad():
            outputs = self.model.language_model.generate(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                use_cache=True
            )
        
        return outputs
    
    def _merge_embeddings(self, text_embeds, vision_embeds, positions):
        """Textì™€ Vision embedding ê²°í•©"""
        # positions: <image> í† í°ì´ ìˆëŠ” ìœ„ì¹˜
        # vision_embedsë¥¼ í•´ë‹¹ ìœ„ì¹˜ì— ì‚½ì…
        
        for i, pos in enumerate(positions):
            text_embeds[:, pos] = vision_embeds[i]
        
        return text_embeds
```

---

### Step 4: í†µì‹  í”„ë¡œí† ì½œ

```python
# API ì„¤ê³„

# Client â†’ Server
{
    "input_ids": [1, 2, 3, ..., 151655, ...],  # 151655 = <image> token
    "vision_embeddings": [[...], [...], ...],  # [num_patches, hidden_dim]
    "vision_token_positions": [10, 11, 12, ...],  # <image> í† í° ìœ„ì¹˜
    "attention_mask": [1, 1, 1, ...],
    "max_new_tokens": 128
}

# Server â†’ Client
{
    "generated_ids": [42, 15, 89, ...],
    "text": "Generated response..."
}
```

---

## íŒŒì¼ êµ¬ì¡°

```
client_inference/
â”œâ”€â”€ README.md (ì´ íŒŒì¼)
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ preprocessor.py        # Step 1: ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ vision_encoder.py      # Step 2: Vision Encoder
â”‚   â”œâ”€â”€ client_api.py          # Client API
â”‚   â””â”€â”€ requirements.txt
â”‚       - qwen-vl-utils
â”‚       - transformers
â”‚       - torch
â”‚       - Pillow
â”‚       - requests
â”‚
â””â”€â”€ server/
    â”œâ”€â”€ llm_inference.py       # Step 3: LLM ì „ìš©
    â”œâ”€â”€ server_api.py          # FastAPI Server
    â””â”€â”€ requirements.txt
        - transformers
        - torch
        - fastapi
        - uvicorn
```

---

## ë‹¤ìŒ ë‹¨ê³„

ì‹¤ì œ ì½”ë“œ êµ¬í˜„ì„ ì›í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”. ë‹¤ìŒì„ ìƒì„±í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. âœ… `client/preprocessor.py` - qwen-vl-utils ê¸°ë°˜ ì „ì²˜ë¦¬
2. âœ… `client/vision_encoder.py` - Vision Encoder ì¶”ì¶œ
3. âœ… `client/client_api.py` - Client í†µí•© API
4. âœ… `server/llm_inference.py` - Server LLM ì „ìš©
5. âœ… `server/server_api.py` - FastAPI ì„œë²„
6. âœ… ì˜ˆì œ ì½”ë“œ

ì–´ë–¤ ë¶€ë¶„ë¶€í„° êµ¬í˜„í• ê¹Œìš”?
