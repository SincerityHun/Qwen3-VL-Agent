# ê¸°ì¡´ ì½”ë“œì—ì„œ ê°€ì ¸ì™€ì•¼ í•  ë¶€ë¶„ - ìƒì„¸ ë§¤í•‘

## ğŸ“‹ ì „ì²´ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CLIENT (On-Device)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Step 1: ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë¡œë“œ & ì „ì²˜ë¦¬                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ qwen-vl-utils/vision_process.py                    â”‚     â”‚
â”‚  â”‚ - fetch_image()  (Line 95-147)                     â”‚     â”‚
â”‚  â”‚ - fetch_video()  (Line 405-478)                    â”‚     â”‚
â”‚  â”‚ - smart_resize() (Line 52-81)                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â†“                                    â”‚
â”‚  Step 2: Tokenization                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ transformers.AutoProcessor                         â”‚     â”‚
â”‚  â”‚ - apply_chat_template()                            â”‚     â”‚
â”‚  â”‚ - tokenizer.encode()                               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â†“                                    â”‚
â”‚  Step 3: Vision Encoder Forward                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ transformers.Qwen3VLForConditionalGeneration       â”‚     â”‚
â”‚  â”‚ â†’ full_model.visual (Vision Encoderë§Œ ì¶”ì¶œ)        â”‚     â”‚
â”‚  â”‚   - patch_embed                                    â”‚     â”‚
â”‚  â”‚   - ViT blocks                                     â”‚     â”‚
â”‚  â”‚   - merger (DeepStack)                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â†“                                    â”‚
â”‚  Step 4: ë°ì´í„° ì „ì†¡                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ - input_ids                                        â”‚     â”‚
â”‚  â”‚ - vision_embeddings                                â”‚     â”‚
â”‚  â”‚ - vision_token_positions                           â”‚     â”‚
â”‚  â”‚ - attention_mask                                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ Network (REST/gRPC)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SERVER (GPU)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Step 5: Vision Embedding ì‚½ì…                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ - Text embeddings: language_model.embed_tokens()   â”‚     â”‚
â”‚  â”‚ - Vision embeddings ë³‘í•©                           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â†“                                    â”‚
â”‚  Step 6: LLM Prefill                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ transformers.Qwen3VLForConditionalGeneration       â”‚     â”‚
â”‚  â”‚ â†’ language_model.forward()                         â”‚     â”‚
â”‚  â”‚   - First forward pass                             â”‚     â”‚
â”‚  â”‚   - KV cache ìƒì„±                                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â†“                                    â”‚
â”‚  Step 7: Auto-regressive Decoding                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ - Next token prediction                            â”‚     â”‚
â”‚  â”‚ - KV cache ì¬ì‚¬ìš©                                  â”‚     â”‚
â”‚  â”‚ - Streaming response                               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ íŒŒì¼ë³„ ìƒì„¸ ë§¤í•‘

### 1ï¸âƒ£ Client - ì „ì²˜ë¦¬ ì½”ë“œ

#### **íŒŒì¼: `qwen-vl-utils/src/qwen_vl_utils/vision_process.py`**

```python
# ============================================
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================

# Line 39-50: í•´ìƒë„ ì¡°ì • ìœ í‹¸ë¦¬í‹°
def round_by_factor(number: int, factor: int) -> int:
    """ê°€ì¥ ê°€ê¹Œìš´ factorì˜ ë°°ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼"""
    return round(number / factor) * factor

def ceil_by_factor(number: int, factor: int) -> int:
    """factorì˜ ë°°ìˆ˜ë¡œ ì˜¬ë¦¼"""
    return math.ceil(number / factor) * factor

def floor_by_factor(number: int, factor: int) -> int:
    """factorì˜ ë°°ìˆ˜ë¡œ ë‚´ë¦¼"""
    return math.floor(number / factor) * factor


# Line 52-81: ë™ì  í•´ìƒë„ ì¡°ì • (í•µì‹¬!)
def smart_resize(
    height: int, 
    width: int, 
    factor: int,  # Qwen3VL: 32, Qwen2.5VL: 28
    min_pixels: Optional[int] = None,
    max_pixels: Optional[int] = None
) -> Tuple[int, int]:
    """
    ì´ë¯¸ì§€ë¥¼ ë‹¤ìŒ ì¡°ê±´ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ:
    1. height, width ëª¨ë‘ factorì˜ ë°°ìˆ˜
    2. ì´ í”½ì…€ ìˆ˜ê°€ [min_pixels, max_pixels] ë²”ìœ„ ë‚´
    3. ì¢…íš¡ë¹„ ìµœëŒ€í•œ ìœ ì§€
    """
    # êµ¬í˜„ ë‚´ìš©...
    return h_bar, w_bar


# Line 84-91: RGB ë³€í™˜
def to_rgb(pil_image: Image.Image) -> Image.Image:
    """RGBA â†’ RGB ë³€í™˜ (íˆ¬ëª… ë°°ê²½ â†’ í°ìƒ‰)"""
    if pil_image.mode == 'RGBA':
        white_background = Image.new("RGB", pil_image.size, (255, 255, 255))
        white_background.paste(pil_image, mask=pil_image.split()[3])
        return white_background
    else:
        return pil_image.convert("RGB")


# Line 95-147: ì´ë¯¸ì§€ ë¡œë“œ & ì „ì²˜ë¦¬ (í•µì‹¬!)
def fetch_image(
    ele: Dict[str, Union[str, Image.Image]], 
    image_patch_size: int = 14  # Qwen3VL: 16
) -> Image.Image:
    """
    ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
    - URL, local path, base64, PIL.Image ì§€ì›
    - ìë™ ë¦¬ì‚¬ì´ì¦ˆ
    """
    # 1. ì´ë¯¸ì§€ ë¡œë“œ
    if isinstance(image, Image.Image):
        image_obj = image
    elif image.startswith("http://") or image.startswith("https://"):
        # URLì—ì„œ ë‹¤ìš´ë¡œë“œ
        response = requests.get(image, stream=True)
        image_obj = Image.open(BytesIO(response.content))
    elif image.startswith("file://"):
        # ë¡œì»¬ íŒŒì¼
        image_obj = Image.open(image[7:])
    elif image.startswith("data:image"):
        # Base64
        _, base64_data = image.split("base64,", 1)
        data = base64.b64decode(base64_data)
        image_obj = Image.open(BytesIO(data))
    else:
        # ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ
        image_obj = Image.open(image)
    
    # 2. RGB ë³€í™˜
    image = to_rgb(image_obj)
    
    # 3. ë¦¬ì‚¬ì´ì¦ˆ
    patch_factor = int(image_patch_size * SPATIAL_MERGE_SIZE)  # 16*2=32
    
    if "resized_height" in ele and "resized_width" in ele:
        # ì‚¬ìš©ì ì§€ì • í¬ê¸°
        resized_height, resized_width = smart_resize(
            ele["resized_height"], ele["resized_width"], factor=patch_factor
        )
    else:
        # min_pixels, max_pixels ê¸°ë°˜ ìë™ ì¡°ì •
        width, height = image.size
        min_pixels = ele.get("min_pixels", IMAGE_MIN_TOKEN_NUM * patch_factor ** 2)
        max_pixels = ele.get("max_pixels", IMAGE_MAX_TOKEN_NUM * patch_factor ** 2)
        resized_height, resized_width = smart_resize(
            height, width, factor=patch_factor,
            min_pixels=min_pixels, max_pixels=max_pixels
        )
    
    image = image.resize((resized_width, resized_height))
    return image


# Line 150-184: ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
def smart_nframes(
    ele: Dict[str, Any],
    total_frames: int,
    video_fps: Union[int, float]
) -> int:
    """
    ë¹„ë””ì˜¤ì—ì„œ ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜ ê³„ì‚°
    - fps ê¸°ë°˜ ë˜ëŠ” nframes ì§ì ‘ ì§€ì •
    """
    if "nframes" in ele:
        nframes = round_by_factor(ele["nframes"], FRAME_FACTOR)
    else:
        fps = ele.get("fps", FPS)  # ê¸°ë³¸ 2.0 FPS
        min_frames = ceil_by_factor(ele.get("min_frames", FPS_MIN_FRAMES), FRAME_FACTOR)
        max_frames = floor_by_factor(ele.get("max_frames", FPS_MAX_FRAMES), FRAME_FACTOR)
        
        nframes = total_frames / video_fps * fps
        nframes = min(min(max(nframes, min_frames), max_frames), total_frames)
        nframes = floor_by_factor(nframes, FRAME_FACTOR)
    
    return nframes


# Line 187-226: Torchvisionìœ¼ë¡œ ë¹„ë””ì˜¤ ì½ê¸°
def _read_video_torchvision(ele: Dict[str, Any]) -> Tuple[torch.Tensor, float]:
    """torchvision.io.read_video ì‚¬ìš©"""
    video_path = ele["video"]
    
    video, audio, info = io.read_video(
        video_path,
        start_pts=ele.get("video_start", 0.0),
        end_pts=ele.get("video_end", None),
        pts_unit="sec",
        output_format="TCHW"
    )
    
    total_frames, video_fps = video.size(0), info["video_fps"]
    nframes = smart_nframes(ele, total_frames, video_fps)
    
    # í”„ë ˆì„ ìƒ˜í”Œë§
    idx = torch.linspace(0, total_frames - 1, nframes).round().long()
    video = video[idx]
    
    return video, video_metadata, sample_fps


# Line 295-337: Decordë¡œ ë¹„ë””ì˜¤ ì½ê¸° (ëŒ€ì•ˆ)
def _read_video_decord(ele: Dict[str, Any]) -> Tuple[torch.Tensor, float]:
    """decord.VideoReader ì‚¬ìš© (ë” ë¹ ë¦„)"""
    import decord
    vr = decord.VideoReader(ele["video"])
    
    total_frames, video_fps = len(vr), vr.get_avg_fps()
    nframes = smart_nframes(ele, total_frames, video_fps)
    
    idx = torch.linspace(0, total_frames - 1, nframes).round().long().tolist()
    video = vr.get_batch(idx).asnumpy()
    video = torch.tensor(video).permute(0, 3, 1, 2)
    
    return video, video_metadata, sample_fps


# Line 405-478: ë¹„ë””ì˜¤ í†µí•© ì²˜ë¦¬ (í•µì‹¬!)
def fetch_video(
    ele: Dict[str, Any],
    image_patch_size: int = 14,
    return_video_metadata: bool = False
) -> Union[torch.Tensor, List[Image.Image]]:
    """
    ë¹„ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
    - í”„ë ˆì„ ìƒ˜í”Œë§
    - ë¦¬ì‚¬ì´ì¦ˆ
    """
    # 1. ë¹„ë””ì˜¤ ì½ê¸° (backend ì„ íƒ)
    video_reader_backend = get_video_reader_backend()
    video, video_metadata, sample_fps = VIDEO_READER_BACKENDS[video_reader_backend](ele)
    
    # 2. í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ
    nframes, _, height, width = video.shape
    
    min_pixels = ele.get("min_pixels", VIDEO_FRAME_MIN_PIXELS)
    total_pixels = ele.get("total_pixels", MODEL_SEQ_LEN * image_factor ** 2 * 0.9)
    max_pixels = max(min(VIDEO_FRAME_MAX_PIXELS, total_pixels / nframes * FRAME_FACTOR), 
                     int(min_pixels * 1.05))
    
    resized_height, resized_width = smart_resize(
        height, width, factor=image_factor,
        min_pixels=min_pixels, max_pixels=max_pixels
    )
    
    # 3. Resize ì ìš©
    video = transforms.functional.resize(
        video, [resized_height, resized_width],
        interpolation=InterpolationMode.BICUBIC, antialias=True
    ).float()
    
    return video, video_metadata


# Line 483-501: Vision ì •ë³´ ì¶”ì¶œ
def extract_vision_info(
    conversations: Union[List[Dict], List[List[Dict]]]
) -> List[Dict[str, Any]]:
    """ë©”ì‹œì§€ì—ì„œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì •ë³´ë§Œ ì¶”ì¶œ"""
    vision_infos = []
    
    for conversation in conversations:
        for message in conversation:
            if isinstance(message["content"], list):
                for ele in message["content"]:
                    if ("image" in ele or "video" in ele or 
                        ele.get("type") in ("image", "video")):
                        vision_infos.append(ele)
    
    return vision_infos


# Line 508-534: í†µí•© ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•µì‹¬!)
def process_vision_info(
    conversations: List[Dict],
    return_video_kwargs: bool = False,
    return_video_metadata: bool = False,
    image_patch_size: int = 14
) -> Tuple[Optional[List[Image.Image]], Optional[List[torch.Tensor]], Optional[Dict]]:
    """
    ëª¨ë“  vision ì •ë³´ ì²˜ë¦¬
    - ì´ë¯¸ì§€ & ë¹„ë””ì˜¤ ë¡œë“œ
    - ì „ì²˜ë¦¬
    """
    vision_infos = extract_vision_info(conversations)
    
    image_inputs = []
    video_inputs = []
    
    for vision_info in vision_infos:
        if "image" in vision_info or "image_url" in vision_info:
            image_inputs.append(fetch_image(vision_info, image_patch_size))
        elif "video" in vision_info:
            video_input, video_metadata, sample_fps = fetch_video(
                vision_info, image_patch_size, return_video_metadata=True
            )
            video_inputs.append((video_input, video_metadata))
    
    return image_inputs, video_inputs, video_kwargs
```

**Clientì—ì„œ í•„ìš”í•œ ê²ƒ:**
- âœ… ìœ„ í•¨ìˆ˜ë“¤ ì „ì²´ ë³µì‚¬
- âœ… Constants (Line 1-37)
- âœ… ì˜ì¡´ì„±: PIL, torch, torchvision, requests

---

### 2ï¸âƒ£ Client - Tokenization

#### **íŒŒì¼: transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**

```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# ============================================
# ì‚¬ìš©í•  ë©”ì„œë“œë“¤
# ============================================

# 1. Chat template ì ìš©
text = processor.apply_chat_template(
    messages,
    tokenize=False,  # ë¨¼ì € í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    add_generation_prompt=True
)
# ì¶œë ¥: "<|im_start|>user\n<image>Describe this image.<|im_end|>\n<|im_start|>assistant\n"

# 2. Tokenization
encoded = processor.tokenizer.encode(text, return_tensors="pt")
# ì¶œë ¥: input_ids = [1, 2, 3, ..., 151655, ...]  # 151655 = <image> token

# 3. í†µí•© ì²˜ë¦¬ (ì´ë¯¸ì§€ í¬í•¨)
inputs = processor(
    text=text,
    images=images,  # PIL.Image ë¦¬ìŠ¤íŠ¸
    videos=videos,  # torch.Tensor
    do_resize=False,  # qwen-vl-utilsì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•¨
    return_tensors="pt"
)
# ì¶œë ¥:
# {
#     'input_ids': tensor([[1, 2, 3, ..., 151655, ...]]),
#     'attention_mask': tensor([[1, 1, 1, ..., 1]]),
#     'pixel_values': tensor([...]),  # [num_images, C, H, W]
#     'image_grid_thw': tensor([[1, 9, 13]]),  # [num_images, 3]
# }
```

**Clientì—ì„œ í•„ìš”í•œ ê²ƒ:**
- âœ… `pip install transformers`
- âœ… Processorë§Œ ë¡œë“œ (ëª¨ë¸ ë¶ˆí•„ìš”)

---

### 3ï¸âƒ£ Client - Vision Encoder

#### **íŒŒì¼: transformers.models.qwen3_vl.modeling_qwen3_vl**

```python
from transformers import Qwen3VLForConditionalGeneration

# ============================================
# Vision Encoder ì¶”ì¶œ
# ============================================

# ì „ì²´ ëª¨ë¸ ë¡œë“œ
full_model = Qwen3VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct",
    torch_dtype="auto",
    device_map="cpu"  # ë˜ëŠ” "cuda"
)

# Vision Encoderë§Œ ì¶”ì¶œ
vision_encoder = full_model.visual  # Qwen3VLVisionModel ê°ì²´

# ëª¨ë¸ êµ¬ì¡°:
# vision_encoder
# â”œâ”€â”€ patch_embed: PatchEmbed
# â”œâ”€â”€ blocks: nn.ModuleList (ViT transformer blocks)
# â”‚   â”œâ”€â”€ block[0]: VisionTransformerBlock
# â”‚   â”œâ”€â”€ block[1]: VisionTransformerBlock
# â”‚   â””â”€â”€ ...
# â””â”€â”€ merger: Qwen3VLMerger (DeepStack)


# ============================================
# Vision Encoder Forward
# ============================================

import torch

@torch.no_grad()
def encode_vision(pixel_values, image_grid_thw):
    """
    Args:
        pixel_values: torch.Tensor [batch, channels, height, width]
        image_grid_thw: torch.Tensor [num_images, 3] - (T, H, W)
    
    Returns:
        vision_outputs: torch.Tensor [num_patches, hidden_dim]
    """
    vision_outputs = vision_encoder(
        pixel_values=pixel_values,
        grid_thw=image_grid_thw
    )
    
    return vision_outputs


# ============================================
# ë‚´ë¶€ ë™ì‘ (ì°¸ê³ ìš©)
# ============================================

class Qwen3VLVisionModel(nn.Module):
    def forward(self, pixel_values, grid_thw):
        # 1. Patch Embedding
        x = self.patch_embed(pixel_values)  # [B, num_patches, embed_dim]
        
        # 2. ViT Blocks
        hidden_states = []
        for i, block in enumerate(self.blocks):
            x = block(x)
            if i in self.merger.layer_indices:  # DeepStackìš©
                hidden_states.append(x)
        
        # 3. DeepStack Merger (multi-level feature fusion)
        merged_features = self.merger(hidden_states)
        
        return merged_features  # [num_patches, hidden_dim]
```

**Clientì—ì„œ í•„ìš”í•œ ê²ƒ:**
- âœ… `full_model.visual` ì¶”ì¶œ
- âœ… ë©”ëª¨ë¦¬: Vision Encoderë§Œ ë¡œë“œ (~2-3GB for 32B modelì˜ vision part)
- âš ï¸ **ë¬¸ì œì **: Vision Encoderë„ í¬ê¸°ê°€ ìˆì–´ì„œ On-Deviceì— ë¶€ë‹´ë  ìˆ˜ ìˆìŒ

---

### 4ï¸âƒ£ Server - LLM ì „ìš©

#### **íŒŒì¼: transformers.models.qwen3_vl.modeling_qwen3_vl**

```python
from transformers import Qwen3VLForConditionalGeneration

# ============================================
# Serverì—ì„œ Vision Embeddings ë°›ì•„ì„œ LLMë§Œ ì‹¤í–‰
# ============================================

class ServerLLMInference:
    def __init__(self, model_name):
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto",
            attn_implementation="flash_attention_2"
        )
        self.model.eval()
    
    @torch.no_grad()
    def generate_from_vision_embeddings(
        self,
        input_ids,  # [batch, seq_len]
        vision_embeddings,  # [num_patches, hidden_dim]
        vision_token_positions,  # <image> í† í° ìœ„ì¹˜
        attention_mask=None,
        max_new_tokens=128
    ):
        """
        Vision embeddingsì„ ë°›ì•„ì„œ LLMë§Œ ì‹¤í–‰
        """
        # 1. Text embeddings
        text_embeds = self.model.language_model.embed_tokens(input_ids)
        # text_embeds shape: [batch, seq_len, hidden_dim]
        
        # 2. Vision embeddings ì‚½ì…
        inputs_embeds = text_embeds.clone()
        
        vision_idx = 0
        for batch_idx in range(input_ids.shape[0]):
            for pos in vision_token_positions[batch_idx]:
                if input_ids[batch_idx, pos] == 151655:  # <image> token
                    # Vision embeddingìœ¼ë¡œ ëŒ€ì²´
                    inputs_embeds[batch_idx, pos] = vision_embeddings[vision_idx]
                    vision_idx += 1
        
        # 3. LLM Forward (Prefill + Decoding)
        outputs = self.model.language_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            use_cache=True,
            do_sample=True,
            temperature=0.7
        )
        
        return outputs


# ============================================
# ê¸°ì¡´ ë°©ì‹ê³¼ ë¹„êµ
# ============================================

# ê¸°ì¡´ (Serverì—ì„œ ì „ì²´ ì²˜ë¦¬):
outputs = model.generate(
    input_ids=input_ids,
    pixel_values=pixel_values,  # Serverì—ì„œ Vision Encoder ì‹¤í–‰
    image_grid_thw=image_grid_thw,
    max_new_tokens=128
)

# ìƒˆë¡œìš´ ë°©ì‹ (Clientì—ì„œ Vision Encoder ì‹¤í–‰):
outputs = model.language_model.generate(
    inputs_embeds=merged_embeddings,  # Vision + Text
    max_new_tokens=128
)
```

**Serverì—ì„œ í•„ìš”í•œ ê²ƒ:**
- âœ… `language_model.generate()` ì‚¬ìš©
- âœ… Vision Encoder ê±´ë„ˆë›°ê¸°
- âœ… Vision embeddings ì§ì ‘ ì‚½ì…

---

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

### Clientì—ì„œ ê°€ì ¸ì™€ì•¼ í•  ì½”ë“œ:

1. **ì „ì²˜ë¦¬ (í•„ìˆ˜):**
   - âœ… `qwen-vl-utils/src/qwen_vl_utils/vision_process.py` ì „ì²´
   - âœ… `transformers.AutoProcessor`

2. **Vision Encoder (ì„ íƒ):**
   - âœ… `full_model.visual` ì¶”ì¶œ
   - âš ï¸ í¬ê¸°: ~2-3GB (32B ëª¨ë¸ ê¸°ì¤€)
   - ğŸ’¡ **ëŒ€ì•ˆ**: Serverì—ì„œ ì‹¤í–‰í•˜ê³  pixel_valuesë§Œ ì „ì†¡

3. **í†µì‹ :**
   - âœ… requests ë˜ëŠ” gRPC
   - âœ… JSON/Protobuf ì§ë ¬í™”

### Serverì—ì„œ ìˆ˜ì •í•  ì½”ë“œ:

1. **Vision Embedding ìˆ˜ì‹ :**
   - âœ… ìƒˆë¡œìš´ ì…ë ¥ íŒŒë¼ë¯¸í„° ì¶”ê°€: `vision_embeddings`
   - âœ… Vision Encoder ê±´ë„ˆë›°ê¸°

2. **LLMë§Œ ì‹¤í–‰:**
   - âœ… `language_model.generate(inputs_embeds=...)`
   - âœ… KV cache í™œìš©

---

## ë‹¤ìŒ ë‹¨ê³„

ì‹¤ì œ êµ¬í˜„ ì½”ë“œë¥¼ ì›í•˜ì‹œë©´ ë‹¤ìŒ ì¤‘ ì„ íƒí•´ì£¼ì„¸ìš”:

1. **Option A: Client ì „ì²˜ë¦¬ + Server (Vision Encoder + LLM)**
   - ê°€ì¥ ê°„ë‹¨
   - ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ìŒ

2. **Option B: Client (ì „ì²˜ë¦¬ + Vision Encoder) + Server LLM**
   - ê· í˜•ì¡íŒ ì ‘ê·¼
   - Vision Encoderë„ On-Device

3. **Option C: Full Client Preprocessing + Server Inference Only**
   - Server ë¶€í•˜ ìµœì†Œí™”
   - Clientì— Vision Encoder í•„ìš”

ì–´ë–¤ ì˜µì…˜ìœ¼ë¡œ ì§„í–‰í• ê¹Œìš”?
