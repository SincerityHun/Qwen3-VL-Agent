# Qwen3-VL Client-Server Deployment Guide

## üìã Overview

This implementation separates Qwen3-VL inference into client and server components:

- **Client**: Preprocessing + Vision Encoder + Gradio UI (Port 7860)
- **Server**: LLM Inference with GPU acceleration (Port 8000)

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Client Container            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ     Gradio Web UI (7860)      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚Üì                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Vision Preprocessing        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (qwen-vl-utils)             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚Üì                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Vision Encoder (ViT)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (2-3GB model)               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚Üì                       ‚îÇ
‚îÇ       vision_embeddings              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ HTTP POST
               ‚îÇ /api/v1/generate
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Server Container            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   FastAPI Server (8000)       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚Üì                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   LLM Inference               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Language Model Only)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   GPU Accelerated             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚Üì                       ‚îÇ
‚îÇ       generated_text                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Prerequisites

- Docker and Docker Compose installed
- NVIDIA GPU with CUDA support (for server)
- NVIDIA Container Toolkit installed

### 1. Build and Run with Docker Compose

```bash
# Build both containers
docker-compose build

# Start services (server first, then client)
docker-compose up -d

# Check logs
docker-compose logs -f

# Access Gradio UI
# Open browser: http://localhost:7860
```

### 2. Environment Configuration

Edit `docker-compose.yml` to customize:

```yaml
services:
  server:
    environment:
      - MODEL_NAME=Qwen/Qwen3-VL-2B-Instruct  # Change model here
      - DEVICE_MAP=auto                       # GPU allocation
      - TORCH_DTYPE=auto                      # fp16/fp32
  
  client:
    environment:
      - SERVER_URL=http://server:8000         # Server endpoint
      - MODEL_NAME=Qwen/Qwen3-VL-2B-Instruct  # Must match server
```

### 3. Manual Build (without Docker Compose)

#### Server
```bash
cd server
docker build -t qwen3vl-server .
docker run --gpus all -p 8000:8000 \
  -e MODEL_NAME=Qwen/Qwen3-VL-2B-Instruct \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  qwen3vl-server
```

#### Client
```bash
cd client
docker build -t qwen3vl-client .
docker run -p 7860:7860 \
  -e SERVER_URL=http://localhost:8000 \
  -e MODEL_NAME=Qwen/Qwen3-VL-2B-Instruct \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  qwen3vl-client
```

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration config
‚îÇ
‚îú‚îÄ‚îÄ client/                     # Client container
‚îÇ   ‚îú‚îÄ‚îÄ README.md        
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py         # Vision preprocessing (qwen-vl-utils)
‚îÇ   ‚îú‚îÄ‚îÄ vision_encoder.py       # Vision Encoder extraction (2048-dim output)
‚îÇ   ‚îú‚îÄ‚îÄ client_api.py           # HTTP client for server
‚îÇ   ‚îú‚îÄ‚îÄ gradio_app.py           # Gradio UI
‚îÇ   ‚îî‚îÄ‚îÄ test_vision_embedding.py # Test vision encoder (images/videos)
‚îÇ
‚îú‚îÄ‚îÄ server/                     # Server container
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ llm_inference.py        # LLM generation logic
‚îÇ   ‚îî‚îÄ‚îÄ server_api.py           # FastAPI endpoints
‚îÇ
‚îú‚îÄ‚îÄ qwen-vl-utils/              # Vision processing utilities (But we just import this package in pip)
‚îÇ   ‚îî‚îÄ‚îÄ src/qwen_vl_utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ vision_process.py   # Image/video loading, resizing
‚îÇ
‚îú‚îÄ‚îÄ cookbooks/                  # Example notebooks
‚îÇ   ‚îú‚îÄ‚îÄ video_understanding.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ ocr.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ README/                     # Documentation
    ‚îú‚îÄ‚îÄ README.md               # Implementation guide
    ‚îî‚îÄ‚îÄ CODE_MAPPING.md         # Source code mapping
```

## üîß API Reference

### Server Endpoints

#### 1. Health Check
```bash
GET /health

Response:
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cuda:0"
}
```

#### 2. Generate (Non-streaming)
```bash
POST /api/v1/generate

Request:
{
  "input_ids": [[1, 2, 3, ...]],
  "vision_embeddings": [[0.1, 0.2, ...], ...],
  "vision_token_positions": [5, 10],
  "attention_mask": [[1, 1, 1, ...]],
  "max_new_tokens": 128,
  "temperature": 0.7,
  "top_p": 0.8
}

Response:
{
  "text": "Generated response...",
  "finish_reason": "stop"
}
```

#### 3. Generate (Streaming)
```bash
POST /api/v1/generate_stream

# Returns newline-delimited JSON (NDJSON)
{"text": "Hello"}
{"text": "Hello world"}
{"text": "Hello world!", "finish_reason": "stop"}
```

## üß™ Testing

### Test Vision Encoder (Client-Side Only)

Test vision preprocessing and encoding without requiring the server:

```bash
cd client

# Test with image
python test_vision_embedding.py image ../examples/dog.jpg

# Test with video
python test_vision_embedding.py video ../examples/IronMan.mp4

# Expected output:
# ‚úÖ Embeddings are non-zero
# ‚úÖ Embeddings are finite
# ‚úÖ Vision encoder hidden dimension is 2048
# ‚úÖ Vision token positions found
```

**Note**: Vision encoder outputs **2048-dim** embeddings, which are later projected to 3584-dim by the LLM.

### Test Server Health
```bash
curl http://localhost:8000/health
```

### Test Client Access
```bash
# Open browser
http://localhost:7860
```

### Test End-to-End
1. Upload an image in Gradio UI
2. Enter prompt: "Describe this image"
3. Click Submit
4. Check server logs: `docker-compose logs server`

## üìä Performance Tuning

### GPU Memory Optimization

For low-memory GPUs, modify server Dockerfile:
```dockerfile
# Use 4-bit quantization
RUN pip install bitsandbytes

# In llm_inference.py
self.model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto"
)
```

### Client-Side Optimization

If Vision Encoder is too large for client devices:
```python
# In client_api.py, send pixel_values instead of embeddings
# Skip vision_encoder.py entirely
```

## üõ†Ô∏è Troubleshooting

### Issue: Server not starting
```bash
# Check GPU availability
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# Check logs
docker-compose logs server
```

### Issue: Client can't connect to server
```bash
# Test server from client container
docker exec qwen3vl-client curl http://server:8000/health

# Check network
docker network inspect qwen3vl_qwen3vl-network
```

### Issue: Out of memory
```bash
# Reduce model size or use quantization
# Set in docker-compose.yml:
environment:
  - MODEL_NAME=Qwen/Qwen3-VL-2B-Instruct  # Use 2B instead of 7B
```

## üìù Development Mode

Run without Docker for development:

### Client Setup
```bash
cd client

# Create virtual environment with uv
uv venv -p 3.12
source .venv/bin/activate

# Install PyTorch with CUDA support
uv pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu128

# Install dependencies
uv pip install -r requirements.txt

# Test vision encoder
python test_vision_embedding.py image ../examples/dog.jpg
python test_vision_embedding.py video ../examples/IronMan.mp4

# Run Gradio app
export SERVER_URL=http://localhost:8000
python gradio_app.py
```

**Important Dependencies:**
- `qwen-vl-utils`: Vision processing utilities
- `decord`: Video decoding (preferred backend, more stable than torchvision)
- `transformers>=4.57.0`: Qwen3-VL model support

### Server Setup
```bash
cd server
pip install -r requirements.txt
python server_api.py
```

### Video Processing Notes

The client uses `qwen-vl-utils` for video processing with the following backends (in order of preference):
1. **decord** (recommended) - Most stable, install with `pip install decord`
2. torchcodec - Experimental
3. torchvision - Deprecated, avoid

If you encounter video processing errors, ensure `decord` is installed in your environment.

## üîê Production Deployment

For production, add:

1. **HTTPS/TLS**: Use nginx reverse proxy
2. **Authentication**: Add API keys to FastAPI
3. **Rate Limiting**: Use FastAPI middleware
4. **Monitoring**: Add Prometheus metrics
5. **Load Balancing**: Deploy multiple server replicas

## üìö References

- [Qwen3-VL Model Card](https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct)
- [qwen-vl-utils Documentation](https://github.com/QwenLM/Qwen-VL)
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Gradio Docs](https://gradio.app/)

## üìÑ License

See LICENSE file in repository root.
